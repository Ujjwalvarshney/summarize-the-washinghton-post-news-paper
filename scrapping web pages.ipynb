{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This we will we will use webscraping to summarise the article in whashinton post\n",
    "process we will first scap the article then devides into sentences removes all the stop word and count the frequency of word in whole sentence and total the frequency of word in each sentences.\n",
    " the sum of words in reach sentence whose total frequency is highest is rank 1  and otherbis ranked 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Admin'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize,word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from heapq import nlargest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrequencySummarizer:\n",
    "    # indentation changes - we are now inside the class definition\n",
    "    def __init__(self, min_cut=0.1, max_cut=0.9):\n",
    "        # The constructor named __init__\n",
    "        # THis function will be called each time an object of this class is \n",
    "        # instantiated\n",
    "        # btw, note how the special keyword 'self' is passed in as the first\n",
    "        # argument to each method (member function).\n",
    "        self._min_cut = min_cut\n",
    "        self._max_cut = max_cut \n",
    "        # Words that have a frequency term lower than min_cut \n",
    "        # or higer than max_cut will be ignored.\n",
    "        self._stopwords = set(stopwords.words('english') + list(punctuation))\n",
    "        # Punctuation symbols and stopwords (common words like 'an','the' etc) are ignored\n",
    "        #\n",
    "        # Here self._min_cut, self._max_cut and self._stopwords are all member variables\n",
    "        # i.e. each object (instance) of this class will have an independent version of these\n",
    "        # variables. \n",
    "        # Note how this function is used to set up the member variables to their appropriate values\n",
    "    # indentation changes - we are out of the constructor (member function, but we are still inside)\n",
    "    # the class.\n",
    "    # One important note: if you are used to programming in Java or C#: if you define a variable here\n",
    "    # i.e. outside a member function but inside the class - it becomes a STATIC member variable\n",
    "    # THis is an important difference from Java, C# (where all member variables would be defined here)\n",
    "    # and is a common gotcha to be avoided.\n",
    "\n",
    "    def _compute_frequencies(self, word_sent):\n",
    "        # next method (member function) which takes in self (the special keyword for this same object)\n",
    "        # as well as a list of sentences, and outputs a dictionary, where the keys are words, and\n",
    "        # values are the frequencies of those words in the set of sentences\n",
    "        freq = defaultdict(int)\n",
    "        # defaultdict, which we referred to above - is a class that inherits from dictionary,\n",
    "        # with one difference: Usually, a Python dictionary throws a KeyError if you try \n",
    "        # to get an item with a key that is not currently in the dictionary. \n",
    "        # The defaultdict in contrast will simply create any items that you try to access \n",
    "        # (provided of course they do not exist yet). THe 'int' passed in as argument tells\n",
    "        # the defaultdict object to create a default value of 0\n",
    "        for s in word_sent:\n",
    "        # indentation changes - we are inside the for loop, for each sentence\n",
    "          for word in s:\n",
    "            # indentation changes again - this is an inner for loop, once per each word_sent\n",
    "            # in that sentence\n",
    "            if word not in self._stopwords:\n",
    "                # if the word is in the member variable (dictionary) self._stopwords, then ignore it,\n",
    "                # else increment the frequency. Had the dictionary freq been a regular dictionary (not a \n",
    "                # defaultdict, we would have had to first check whether this word is in the dict\n",
    "                freq[word] += 1\n",
    "        # Done with the frequency calculation - now go through our frequency list and do 2 things\n",
    "        #   normalize the frequencies by dividing each by the highest frequency (this allows us to \n",
    "        #            always have frequencies between 0 and 1, which makes comparing them easy\n",
    "        #   filter out frequencies that are too high or too low. A trick that yields better results.\n",
    "        m = float(max(freq.values()))\n",
    "        # get the highest frequency of any word in the list of words\n",
    "        \n",
    "        for w in list(freq.keys()):\n",
    "            # indentation changes - we are inside the for loop\n",
    "            freq[w] = freq[w]/m\n",
    "            # divide each frequency by that max value, so it is now between 0 and 1\n",
    "            if freq[w] >= self._max_cut or freq[w] <= self._min_cut:\n",
    "                # indentation changes - we are inside the if statement - if we are here the word is either\n",
    "                # really common or really uncommon. In either case - delete it from our dictionary\n",
    "                del freq[w]\n",
    "                # remember that del can be used to remove a key-value pair from the dictionary\n",
    "        return freq\n",
    "        # return the frequency list\n",
    "\n",
    "    def summarize(self, text, n):\n",
    "        # next method (member function) which takes in self (the special keyword for this same object)\n",
    "        # as well as the raw text, and the number of sentences we wish the summary to contain. Return the \n",
    "        # summary\n",
    "        sents = sent_tokenize(text)\n",
    "        # split the text into sentences\n",
    "        assert n <= len(sents)\n",
    "        # assert is a way of making sure a condition holds true, else an exception is thrown. Used to do \n",
    "        # sanity checks like making sure the summary is shorter than the original article.\n",
    "        word_sent = [word_tokenize(s.lower()) for s in sents]\n",
    "        # This 1 sentence does a lot: it converts each sentence to lower-case, then \n",
    "        # splits each sentence into words, then takes all of those lists (1 per sentence)\n",
    "        # and mushes them into 1 big list\n",
    "        self._freq = self._compute_frequencies(word_sent)\n",
    "        # make a call to the method (member function) _compute_frequencies, and places that in\n",
    "        # the member variable _freq. \n",
    "        ranking = defaultdict(int)\n",
    "        # create an empty dictionary (of the superior defaultdict variety) to hold the rankings of the \n",
    "            # sentences. \n",
    "        for i,sent in enumerate(word_sent):\n",
    "            # Indentation changes - we are inside the for loop. Oh! and this is a different type of for loop\n",
    "            # A new built-in function, enumerate(), will make certain loops a bit clearer. enumerate(sequence), \n",
    "            # will return (0, thing[0]), (1, thing[1]), (2, thing[2]), and so forth.\n",
    "            # A common idiom to change every element of a list looks like this:\n",
    "            #  for i in range(len(L)):\n",
    "            #    item = L[i]\n",
    "            #    ... compute some result based on item ...\n",
    "            #    L[i] = result\n",
    "            # This can be rewritten using enumerate() as:\n",
    "            # for i, item in enumerate(L):\n",
    "            #    ... compute some result based on item ...\n",
    "            #    L[i] = result\n",
    "            for w in sent:\n",
    "                # for each word in this sentence\n",
    "                if w in self._freq:\n",
    "                    # if this is not a stopword (common word), add the frequency of that word \n",
    "                    # to the weightage assigned to that sentence \n",
    "                    ranking[i] += self._freq[w]\n",
    "        # OK - we are outside the for loop and now have rankings for all the sentences\n",
    "        sents_idx = nlargest(n, ranking, key=ranking.get)\n",
    "        # we want to return the first n sentences with highest ranking, use the nlargest function to do so\n",
    "        # this function needs to know how to get the list of values to rank, so give it a function - simply the \n",
    "        # get method of the dictionary\n",
    "        return [sents[j] for j in sents_idx]\n",
    "       # return a list with these values in a list\n",
    "# Indentation changes - we are done with our FrequencySummarizer class!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement urllib2 (from versions: none)\n",
      "ERROR: No matching distribution found for urllib2\n"
     ]
    }
   ],
   "source": [
    "! pip install urllib2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now get urrl and summarize \n",
    "import urllib\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request as urllib2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_only_text_washington_post_url(url):\n",
    "    page = urllib2.urlopen(url).read().decode('utf8')\n",
    "    # download the url\n",
    "    soup = BeautifulSoup(page)\n",
    "    text = ' '.join(map(lambda p: p.text,soup.find_all('article')))\n",
    "    soup2 = BeautifulSoup(text)\n",
    "    text = ' '.join(map(lambda p: p.text , soup2.find_all('p')))\n",
    "    return soup.title.text ,text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "someUrl = \"https://www.washingtonpost.com/politics/trump-says-his-economic-policies-have-been-good-for-african-americans-look-closer/2020/07/12/81d5776e-c2be-11ea-864a-0dd31b9d6917_story.html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the article we would like to summarize\n",
    "textOfUrl = get_only_text_washington_post_url(someUrl)\n",
    "# get the title and text\n",
    "fs = FrequencySummarizer()\n",
    "# instantiate our FrequencySummarizer class and get an object of this class\n",
    "summary = fs.summarize(textOfUrl[1], 5)\n",
    "# get a summary of this article that is 3 sentences long\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Subscribe today.arrow-right“From the day I took office, I have been working to build an unlimited future for African American communities who have given, served, and sacrificed so much for our nation,” he said, pointing to record low unemployment rates.And “really happily for me,” the president trumpeted this month, was the “700,000 jobs for African American workers added in the last two months.”Yet, even as monthly black jobless rates were falling to 5.4 percent in August 2019, continuing a drop that began years earlier under President Barack Obama, Trump administration policies and programs — notably regarding the Minority Business Development Agency and the Small Business Administration — did not advance black economic development.ADADThen, as the United States attempted its latest reckoning with centuries of systemic racism, it became clear that “The Coronavirus Crisis Is Worsening Racial Inequality.” That’s the title of a paper from the Center for American Progress, a liberal think tank, outlining ways administration policies and programs work against African American financial progress.Rather than a “great equalizer,” as New York Gov.',\n",
       " 'Nearly one-quarter of African American homeowners and 45 percent of black renters, both at least twice the white rates, “have slight or no confidence that they will be able to make next month’s payment,” he wrote, and “27 percent of Black respondents reported that they are currently experiencing food insecurity.”  That’s three times the white rate.ADAD●“The number of active business owners in the United States plummeted by 3.3 million or 22 percent over the crucial two-month window from February to April 2020,” according to the National Bureau of Economic Research, a private, nonpartisan organization that is the official recession arbiter.',\n",
       " '“Had the Trump administration prioritized the economic well-being of all Americans\\xa0instead of just wealthy investors, black Americans would likely be in a better economic situation than they are now.”ADADExamples of the disparities and related Trump policies include:●The administration repeatedly has proposed devastating budget cuts for the Commerce Department’s Minority Business Development Agency.',\n",
       " 'Those 2.1 percentage points equal a drop of just over one-fourth.The virus-induced black jobless rate shot up to 16.8 percent in May and was 15.4 percent in June, about one-third higher than the white unemployment figure.“This is a tough situation and it’s especially tough for black Americans,” Morial said.',\n",
       " 'Trump’s fiscal 2021 budget request would slash its funding by 76 percent, which would emaciate the agency’s mission to promote the growth of minority-owned businesses.●Despite vocal support from Trump for small, black-owned businesses, only 3 percent of funding through the Small Business Administration’s primary loan program goes to African Americans, who are 13 percent of the population.']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
